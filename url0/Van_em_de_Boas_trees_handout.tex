\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epsfig}
%\usepackage{newalg}
\usepackage{graphicx}
\usepackage{pst-tree,pst-node}
\usepackage{fancyhdr,hyperref,url}
%\usepackage{psfig}

\textwidth 7in
\textheight 9.5in

\pdfpagewidth 8.5in
\pdfpageheight 11in 

\newcommand{\proc}[1]{\textnormal{\scshape#1}}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf 6.851: Advanced Data Structures } \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Scribe: #4}{Lecture #1}}

\pagestyle{fancy}

\lhead{CS 356T}
\rhead{Fall 2019}
\chead{Week 4: Predecessors and van Emde Boas trees}
\cfoot{\thepage}
\renewcommand{\footrulewidth}{0.4pt}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{question}{Question}
\newtheorem{problem}{Problem}
\newtheorem{remark}{Remark}

\newcommand{\U}{\mathcal{U}}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\begin{document}

\begin{remark}
This handout contains some notes from the Advanced Data Structures Course at MIT taught by Erik Demaine and Oren Weimann.  I will intersperse questions and problems throughout the text.  My comments and clarifications will always appear as {\em remarks} to help distinguish them from the course notes.
\end{remark}

\begin{remark}
In the last handout, you solved several challenging technical problems.  This week, we will diverge somewhat from problems that push us technically to readings that push us technically.  In this sense, our goal is comprehension and the questions and the problems will expand the ideas given in the text, rather than develop new or different ideas.  The readings will be technical, so you should be prepared to discuss the key ideas during our tutorial session. 
\end{remark}

\begin{question}
Last week we studied {\em treaps}, a randomized alternative to binary search trees, and a data structure that efficiently solves the predecessor problem under the comparison model.  This week we consider again the question of computing models.  We know a Turing machine is a good model for studying macro-level complexity since it can emulate most other reasonable models of computation up to a polynomial factor.  However, when we talk about polynomial differences in algorithms, the Turing machine model completely breaks down.  Why is this?
\end{question}

\lecture{12 --- March 21, 2007}{Spring 2007}{Oren Weimann}{Tural Badirkhanli}

\section{Overview}

Today we start the topic of integer data structures. We first specify the model of computation we are going to use and then beat the usual $O(\lg{n})$ bounds for $insert/delete$ and $successor/predecessor$ queries using van Emde Boas and y-fast trees. We assume that the elements -- inputs, outputs, memory cells -- are all $w$ bit integers where $w$ is the word size. We also assume a fixed size universe $\U = \{0, 1, \ldots ,u-1\}$ where $u = 2^w$. 

\begin{remark}
The word size on most computing platforms is either 32 or 64 bits.
\end{remark}

\section{Models of Computation}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.7]{models_of_computation_fig}
\end{center}
\end{figure}

{\em Cell Probe Model: } This is the strongest model, where we only pay for accessing memory (reads or writes) and any additional computation is free. Memory cells have some size $w$, which is a parameter of the model. The model is non-uniform, and allows memory reads or writes to depend arbitrarily on past cell probes. Though not realistic to implement, it is good for proving
lower bounds.

\begin{remark}
Uniform models of computation essentially specify an algorithm for all input sizes, where as non-uniform models of computation give potentially different algorithms for different input sizes.  Think about the differences between Turing machines and circuits.  The Turing machine takes any input size.  A circuit is bounded by its input gates, which is why people often talk about a family of circuits $C=(C_{1}, \ldots, C_{n})$.
\end{remark}

\begin{question}
Why is the cell probe model good for proving lower bounds?
\end{question}

{\em Transdichotomous RAM Model: } This model tries to model a realistic computer. We assume $w \geq \lg{n}$; this means that the ``computer'' changes with
the problem size. However, this is actually a very realistic assumption: we always assume words are large enough to store pointers and indices into the data, since otherwise we cannot even address the input. Also, the algorithm can only use a finite set of operations to manipulate data. Each operation can only manipulate $O(1)$ cells at a time. Cells can be addresses arbitrarily; ``RAM'' stands for Random Access Machine, which differentiates the model from classic but uninteresting computation models such as a tape-based Turing Machine.

\begin{remark}
Fredman and Willard coined the term {\em transdichotomous RAM} ``because the dichotomy between the machine model and the problem size is crossed in a reasonable way.''
%~\cite{benoit-et-al:algorithmica2005}.
\end{remark}

Depending on which operations we allow, there are several instantiations
of the {\em Transdichotomous RAM Model}:

\begin{itemize}

\item {\em Word RAM: } In this model we have {\em Transdichotomous RAM Model} $ + O(1)$ ``C-style'' operations like \verb!+ - * / \% & | ^ ~ << >>!. This is the model we are going to use today. 

\item {\em AC$^0$ RAM: } The operations in this model must have an implementation by a constant-depth, unbounded fan-in, polynomial-size (in $w$)
  circuit. Practically, it allows all the operations of word
  RAM except for multiplication in constant time.

\item {\em Pentium RAM} -- While interesting practically, this model
  is of little theoretical interest as it tends to change over time.
\end{itemize}

\begin{question}
Defend the choice of a Word RAM model as a viable model for algorithm design and analysis.
\end{question}

{\em Pointer Machine Model: }  In this model the data structure is described by a directed graph with constant branching factor. For the fixed universe there is a pointer to each element in the universe $\U$. The input to an operation is just one of these pointers.


\section{Successor/Predecessor Problem}

The goal is to maintain a set $S$ of $n$ items from an ordered universe $\U$ of size $u$. The elements are integers that fit in a machine word, that is, $u = 2^w$. Our data structure must support the following operations:

\begin{itemize}

\item $\proc{insert}(x, S),\quad x \in \U$

\item $\proc{delete}(x, S),\quad x \in S$ and $\quad x \in S$

\item $\proc{successor}(x),\quad x \in \U$

\item $\proc{predecessor}(x),\quad x \in \U$

\end{itemize}


\begin{picture}(420,30)

\put(10,20){\line(1,0){400}}
\put(17,12){\line(0,1){16}}
\put(71,12){\line(0,1){16}}
\put(111,12){\line(0,1){16}}
\put(220,12){\line(0,1){16}}
\put(323,12){\line(0,1){16}}
\put(380,12){\line(0,1){16}}

\put(150,15){\Huge{x}}

\put(100,5){$\proc{pred}(x)$}
\put(200,5){$\proc{succ}(x)$}

\end{picture}

As you can see, we have some universe $\U$ (the entire line) and some
set $S$ of points. It is important to note that the \proc{predecessor}
and \proc{successor} functions can take any element in $\U$, not just
elements from $S$. For our model of computation we use the {\em Word RAM Model} described above.

\subsection{Classic Results}

\begin{table}[htb]
\begin{tabular}{ccccc}
  & data structure & time/op & space & model \\
  \hline
  1962 & balanced trees & $O(\lg{n})$ & $O(n)$ & BST \\
  1975 & van Emde Boas \cite{Boas} & $O(\lg{w})=O(\lg\lg{u})$ & $O(u)$ & word / AC$^0$
  RAM \\
  1984 & y-fast trees \cite{Willard} & $O(\lg{w})$ w.h.p. & $O(n)$ & word
  RAM \\
  1993 & fusion trees \cite{Fredman} & $O(\lg_w{n})=O\left(\dfrac{\lg{n}}{\lg\lg{u}}\right)$ & $O(n)$ &
  word RAM; also AC$^0$ RAM \cite{Thorup}  \\ %???  
\end{tabular}
\end{table}

These are the classic solutions to the $successor/predecessor$ problems up to
about 1993. Today we present van Emde Boas and y-fast trees. 
In the following lectures, we will
discuss more recent results not mentioned in this table.


\subsection{Combination of classic results}
Fusion trees work well when the size of the universe is much bigger
than the size of the data or $\lg{\lg u} \geq \sqrt{\lg n}$, while van Emde Boas and y-fast trees work well when the size of the universe is much closer to the size of the data, or $\lg{\lg{u}} \le \sqrt{\lg n}$ . We can therefore choose which data structure to use
depending on the application. They are equivalent around when
$\Theta(\lg{w}) = \Theta\left( \dfrac{\lg{n}}{\lg{w}}\right)$, so that
is the worst case in which we would use either data structure. In this
worst case, we notice that $\lg{w}=\Theta(\sqrt{\lg{n}})$, so we can
always achieve $O(\sqrt{\lg{n}})$, which is significantly better than
the simpler BST model.

\begin{remark}
At this point, you should read the very detailed description and analysis of van Emde Boas trees given by Erik Demaine in the 2003 version of his Advanced Data Structures course.  We will call these the {\em vEB notes}.  The vEB notes are available on the course website.  The following problems and questions are all with respect to the vEB notes.
\end{remark}

\begin{problem}
Given some element $x \in \mathcal{U}$ composed of $w=\lg u$ bits, we define $high(x)$ to be the high-order half of the bits representing $x$ and $low(x)$ to be the low-order half of the bits representing $x$.  Mathematically, these were defined as:
\begin{itemize}
	\item $high(x) = \lfloor x / \sqrt{u} \rfloor$
	\item $low(x) = x \mod \sqrt{u}$
\end{itemize}
Argue why these mathematical definitions make sense.
\end{problem}

\begin{problem}
In computer science, we like to think of the logarithm computationally as successive division by 2.  Thus, $\lg n$ is the number of times you can successively divide before falling below 1.  Under this model, how should we think of $\lg \lg n$?
\end{problem}

\section{Theory versus Practice}

We will again take a look at theory versus practice.  Begin by reading the following paper which is available for download off the course website:

{\em Degermark, M., Brodnik, A., Carlsson, S., and Pink, S. 1997. Small forwarding tables for fast routing lookups. 
SIGCOMM Comput. Commun. Rev. 27, 4 (Oct. 1997), 3-14.}

\begin{question}
Write a short review of the {\em Degermark et al.} paper.  Make sure to address the importance of the engineering of the van Emde Boas structure.
\end{question}

Last week we implemented a treap which has $O(\log n)$ insert, delete, predecessor, successor, and membership test operations.  This week, I would like you to implement a van Emde Boas tree and again do some comparisons with Java's {\tt HashSet}.  Here are a few notes:
\begin{itemize}

	\item When working in the Word RAM model, we assume that both arithmetic operations on integers and direct addressing (i.e. array lookups) run in $O(1)$ time.  Be careful not to break this assumption by performing operations on the integer which may take more time (for example, performing $w/2$ shifts where $w$ is the word size immediately breaks the stated $O(\lg w)$ goal).
	\item The van Emde Boas tree has a recursive structure, so your class will contain an array of smaller van Emde Boas trees (the substructures) as well as a single summary van Emde Boas tree.  You'll also need to record the minimum and maximum element as well as the universe size.  You may prefer to store the {\em width} of the universe ({\em i.e.} the number of bits necessary to represent the universe) since it will make computing $high$ and $low$ easier.
	\item Make sure that when your universe is an odd power of two that your $high$ and $low$ functions work appropriately.
	
\end{itemize}


\begin{problem}
Implement a van Emde Boas tree in Java.  Call your class {\tt VEBTree} and make sure that it accepts an integer argument in the constructor that determines the size of the universe (i.e. the largest value allowable in your set).  Provide methods for {\tt add}, {\tt ceiling}, {\tt remove}, {\tt floor}, and {\tt contains}.  No need to make the class generic since van Emde Boas trees operate exclusively on integers.  Your {\tt ceiling} and {\tt floor} functions should be exclusive so that {\tt ceiling} always returns the smallest number strictly larger than the query and {\tt floor} always returns the largest number strictly smaller than the query.  Now, run an experiment comparing the performance of your {\tt VEBTree} with your {\tt Treap}. [If you did not manage to get your implementation of Treap to support the operations needed for this exercise, then compare your {\tt VEBTree} to Java's {\tt TreeSet} structure.]  Create a {\tt VEBTree} with a universe size of {\tt Integer.MAX\_VALUE / 64}.  Perform 5-10 million integer inserts (randomly pre-compute a sequence of 5-10 million unique integers).  Record the time to insert 1000 integers at a time (then divide by 1000) to avoid timing issues.  Do the same experiment on your treap.  Now plot insertion time as a function of the size of your data structure but make the size dimension a log plot (i.e. the horizontal-axis is $\log(n)$ instead of $n$).  Note that the van Emde Boas tree has a fixed size universe, so its running time should be a line with no slope.  Let me know if you encounter memory problems. 
\end{problem}

\bibliographystyle{alpha}

\begin{thebibliography}{77}

\bibitem{Boas}
P. van Emde Boas, 
\emph{Preserving Order in a Forest in less than Logarithmic Time},
FOCS, 75-84, 1975.

\bibitem{Willard}
Dan E. Willard,
\emph{Log-Logarithmic Worst-Case Range Queries are Possible
in Space $\Theta(n)$},
Inf. Process. Lett. 17(2): 81-84 (1983)

\bibitem{Fredman}
M. Fredman, D. E. Willard,
\emph{Surpassing the Information Theoretic Bound with Fusion Trees},
J. Comput. Syst. Sci, 47(3):424-436, 1993. 

\bibitem{Thorup}
A. Andersson, P. B. Miltersen, M. Thorup,
\emph{Fusion Trees can be Implemented with AC$^{\mbox{0}}$ Instructions
  Only},
Theor. Comput. Sci, 215(1-2): 337-344, 1999.




\end{thebibliography}


\end{document}
