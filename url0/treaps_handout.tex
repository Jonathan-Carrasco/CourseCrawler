\documentclass[11pt]{article}

\usepackage{times,graphicx,epstopdf,fancyhdr,amsfonts,amsthm,amsmath,algorithm,algorithmic,xspace,hyperref}
\usepackage[left=.75in,top=.75in,right=.75in,bottom=.75in]{geometry}

\textwidth 7in
\textheight 9.5in

\pdfpagewidth 8.5in
\pdfpageheight 11in 

\pagestyle{fancy}

\lhead{CS 356T}
\rhead{Fall 2019}
\chead{Week 3: Predecessors and Treaps}
\cfoot{\thepage}
\renewcommand{\footrulewidth}{0.4pt}

\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{question}{Question}
\newtheorem{problem}{Problem}

\begin{document}

\section{The Plan}
These notes will guide you through the basic ideas.  You should also read (at least) Sections 1-5 of
Seidel and Aragon's paper on {\em randomized search trees}, linked to from the course home page.
This week, student A will report on the most interesting aspects of the problems and coding assignment,
while student B will summarize the key elements the Seidel and Aragon paper.

\section{The Predecessor Problem}

Let $S$ be a set of distinct numbers\footnote{This will make our analysis easier later}.  Given some number $x$, the {\em predecessor} of $x$ in $S$ is the largest element in $S$ no exceeding  $x$.  Mathematically, we write

\[
pred_{S}(x) = \max \{y \in S \,|\, y \leq x\}.
\]

Another way to think about predecessors is to imagine the infinite real number line.  Moving left gives smaller numbers while moving right gives larger numbers.  Suppose you make a mark on the line at every position given in $S$.  Now, given a number $x$, find $x$ on the line and move left until you hit a mark.  There is no need to move if you're already on a mark.  This mark gives the predecessor of $x$ in $S$.

%\begin{center}
%\includegraphics[width=.5\linewidth]{figures/predecessor-line}
%\end{center}
 
Given a set of numbers $S$ the {\em predecessor problem} asks you to preprocess $S$ to efficiently answer predecessor queries for any number $x$.  That is, organize $S$ so that  $\mbox{pred}_{S}(x)$ is efficient for all $x$.

\begin{question}
What is a good way to solve the predecessor problem for a fixed set of numbers $S$?
\end{question}

\begin{question}
What does {\em efficient} mean with respect to the predecessor problem?  Why is your definition of efficient good?  
\end{question}

Often we consider the predecessor problem when the set of numbers is dynamic.  In this case we allow the insertion and deletion of elements in $S$.  {\em Binary search trees} provide a solution to the predecessor problem on dynamic sets.  A binary search tree for a set of elements $S$ is a binary tree whose internal nodes contain the elements of $S$ and whose leaves (for convenience) contain the null value.  In addition, a binary search tree exhibits the {\em search tree property} which says that if $t$ is an internal node of the the search tree with value $x$ then all elements in the left subtree of $t$ have value less than $x$ and all elements in the right subtree of $t$ have value greater than $x$.  Note here that we are relying on our assumption that $S$ is always a set of distinct numbers.  Binary search trees support {\em insert}, {\em delete}, and {\em search} operations that take time linear in the {\em height} of the tree.  Of course, a sequence of these operations may cause the tree to become very imbalanced so that the height is proportional to the size of $S$.  

\begin{question}
What is the maximum height of a binary search tree after $n$ numbers have been inserted into it?  What sequence of numbers causes this worst-case behavior?  
\end{question}

Because of this poor-worst case performance, the predecessor problem on dynamic sets is usually solved with {\em balanced binary search trees} like AVL-trees, red-black trees and B-trees.  These trees use tricks like rotations to keep the tree balanced.  You probably have seen some of these trees previously in a data structures course.  Balanced binary trees are pervasive.  For example, red-black trees serve as the underlying data structure for the {\tt set} class in the Standard Template Library in C++ and the {\tt TreeMap} and {\tt TreeSet} class in the Java Collections Framework.

Balanced binary trees support $O(\log n)$-time insertion, deletion, successor, predecessor, and search operations.  However, most of these data structures are tedious to implement and the constant factors hidden inside the asymptotic notation can sometimes be quite large.

The good news is that if we use a small of amount of randomness, we can design a data structure for the predecessor problem that maintains the logarithmic bounds on the operations in expectation with very high probability.  High probability here means that is is more likely for the computer to exhibit hardware failure while simultaneously being hit by an astroid than it is for the operation to take non-logarithmic time.  Well, not actually, what it really means is that the probability the tree has depth larger than $c\log n$ is less than $n^{-d}$ where $d$ is a constant that depends on $c$.

Finally, a note on $S$.  We assume that $S$ is a set of numbers.  We do this because it's easy.  Often $S$ is a set of objects where each $x \in S$ has some key $key(x)$ associated with it.  We use the keys to index the elements in the search trees instead of the elements themselves.  However, for convenience we often use $x$ in place of $key(x)$ without confusion.

\begin{question}
Given a binary search tree $T$ for the set $S$ and some number $x$, how does one find $pred_{S}(x)$ in $T$?  How about the successor?
\end{question}

\begin{question}
Suppose that you were not interested in finding the predecessor or the successor of an item, and instead, only interested in search, insertion, and deletion.  This is known as the {\em dynamic dictionary problem}.  What is a good way to solve the dynamic dictionary problem?
\end{question}

\section{Treaps}

Many of you might have seen Skip Lists as a solution to the predecessor problem on dynamic sets.  A Skip List is a randomized data structure that supports dictionary operations and predecessor / successor searches in $O(\log n)$ time with high probability.  This week, we will look at another randomized data structure, the Treap, which also supports dynamic dictionary operations and predecessor / successor searches in $O(\log n)$ time with high probability.

To start, recall that a binary tree $T$ exhibits the {\em heap property} if, whenever $t$ is an internal node with value $x$, all the elements in both the left and right subtrees of $t$ have values larger than $x$.  Treaps are binary search trees that maintain the heap property.  Thus, a treap is a {\em portmanteau}\footnote{Other examples of portmanteaus are {\em TomKat}, {\em Brangelina}, and {\em gerrymander}} of tree and heap.    The idea is that each element $x \in S$ has a distinct priority, denoted $priority(x)$, drawn uniformly at random from the reals.\footnote{In reality, one can use any subinterval of the reals like (0,1)} A treap maintains the search tree property with respect to the keys of $S$ and the heap property with respect to the priorities of $S$.  Colloquially we say that if $x$ has higher priority than $y$ that $priority(x) < priority(y)$.  This is annoying since it can create confusion when doing comparisons.  Here when we say $x$ has a higher priority than $y$ we literally mean that $priority(x) > priority(y)$.  This means that the element with smallest priority occupies the root of the tree.

\begin{problem}
Let $S$ be a set of elements where each element has a distinct key and a distinct priority.  Use induction to show that there is exactly one treap for $S$.
\end{problem}

\subsection{Insertion and Deletion}

Insertion and deletion in treaps is pretty easy.  To insert a new element, we first assign it a random priority, then we use the standard binary search tree insertion algorithm:  walk down the tree, moving left if the key value is less than the current node and moving right if the key value is larger than the current node, finally inserting the new element at a leaf in the appropriate spot.  Of course, this breaks the heap property, so now you need to rotate the node up with respect to its priority while still maintaining the search-tree property of the tree.  You may want to review binary search tree insertion and floating heap elements up if this does not make sense.  Notice that the insertion operation is linear in the height of the tree.  

\begin{problem}
Define a deletion operation for the treap data structure.  Its running time should be proportional to the height of the tree.  As a hint, think about performing insertion backwards.
\end{problem}

\begin{problem} \label{prob:split}
Let $T$ be a treap on a set of $n$ elements.  Describe an operation $split(T,k)$ that splits $T$ into two treaps $T_{1}$ and $T_{2}$ where $T_{1}$ contains all the elements from $T$ with key values at most $k$ and $T_{2}$ contains all the elements from $T$ with key values larger than $k$.  Prove that your algorithm runs in time proportional to the height of $T$.
\end{problem}

\subsection{Analysis}

Since a treap is just a binary search tree, we might not expect it to behave any better than an unbalanced binary search tree---it might have bad worst-case performance.  However, this is not the case in expectation.  Here's some intuition:  Imagine that, given a set of distinct numbers $S$, you insert the numbers from $S$ into a standard binary search tree, but that you do so by choosing a number uniformly at random from the remaining elements in $S$.  In this case, the binary search tree is very likely balanced.  In fact, the expected height of the tree is $O(\log n)$ with high probability.  This is not surprising given that this process precisely mimics the process of randomized quicksort---the random element behaves like a random pivot.  Assigning a random priority to each element before inserting it into the treap means that each element is equally likely to be the root.  So it too mimics the randomized quicksort algorithm.  The analysis below is inspired by this observation.

We will show that a treap is well-balanced in expectation\footnote{This analysis is derived from Mark de Berg's notes and Motwani and Raghavan's Randomized Algorithms text.  M\&R is on reserve in Schow Library.}.  Given a treap $T$ and an element $x$, let $depth(x)$ be the depth of $x$ in $T$.  The depth of an element is its distance from the root.   So the root has depth 0, the children of the root each have depth 1, and so on.  Order the elements in $S$ by their rank so that $x_{1} < x_{2} < x_{3} < \cdots < x_{n}$.  We will show that the expected depth of any element $x_{k}$ is at most $2\log n$.  Note that $depth(x_{k})$ is a random variable since its insertion is governed by a random process.  

\begin{lemma}
$E[depth(x_{k})] < 2 \log n$
\end{lemma}

\begin{proof}
The value of $depth(x_{k})$ is determined by the number of elements that are ancestors of it in $T$.  Let $X_{i}$ be an indicator random variable so that $X_{i}=1$ if and only if element $x_{i}$ is an ancestor of $x_{k}$ in $T$.  Then
\[
E[depth(x_{k})] = E[\sum_{i=1}^{n} X_{i}] = \sum_{i=1}^{n}E[X_{i}] =  \sum_{i=1}^{n} Pr[X_{i} = 1]
\]
where the penultimate equality uses the linearity of expectation (i.e., the expectation of a sum is the sum of the expectations) and the final equality follows from the definition of expectation.  This leaves us with the problem of computing the probability that $x_{i}$ is an ancestor of $x_{k}$.  Order the elements by their rank so $x_{1} < x_{2} < \ldots x_{n}$.  We consider two cases:  when $i < k$ and when $i > k$.

\begin{question}
Why do we ignore the case where $i=k$?
\end{question}

\begin{claim} \label{claim:ancestor}
If $i < k$ then $x_{i}$ is an ancestor of $x_{k}$ if and only if $priority(x_{i}) < priority(x_{j})$ for all $i < j \leq k$.  If $i > k$ then $x_{i}$ is an ancestor of $x_{k}$ if and only if $priority(x_{i}) < priority(x_{j})$ for all $k \leq j < i$. 
\end{claim}
 
\begin{problem}
Prove Claim~\ref{claim:ancestor}.
\end{problem}

Assuming that Claim~\ref{claim:ancestor} holds then the probability that $x_i$ is an ancestor of $x_k$ is always $1/(k-i+1)$ when $i<k$ and $1/(i-k+1)$ when $i>k$.  This is true because the priorities are assigned uniformly at random so each element has an equal chance of having the smallest priority.  Now we have
 
 \begin{eqnarray*}
 E[depth(x_{k})] & = &  \sum_{i=1}^{n} Pr[X_{i} = 1] \\
 & = & \sum_{i=1}^{k-1} Pr[\mbox{$x_{i}$ is an ancestor of $x_{k}$}] + \sum_{i=k+1}^{n} Pr[\mbox{$x_{i}$ is an ancestor of $x_{k}$}] \\
 & = & \sum_{i=1}^{k-1} 1/(k-i+1) + \sum_{i=k+1}^{n} 1/(i-k+1) \\
 & = & \sum_{i=2}^{k} 1/i + \sum_{i=2}^{n-k+1} 1/i \\
 & = & H_{k} + H_{n-k+1} \qquad\mbox{where $H_{t} = \sum_{j=1}^{t} 1/j = \mbox{the $t^{th}$ harmonic number} < \ln t + 1$} \\
 & < & 2+ \ln k + \ln (n-k+1) \\
 & < & 2\log n
 \end{eqnarray*}
 \end{proof}

Unfortunately, the fact that the expected depth of an element is $O(\log n)$ does not imply that the expected depth of all the elements is $O(\log n)$ simultaneously.  For this, we need to quantify the actual probability that a node has depth $O(\log n)$.  In other words, we will show that the expected depth of an element occurs with high probability.  Remember that we expressed the depth of a node $x_{k}$ by the sum of its indicator random variables $X_{i}$ where $X_{i}=1$ if and only if $x_{i}$ is an ancestor of $x_{k}$.  

\begin{question}
Argue that the indicator random variables for a fixed $x_{k}$ are independent of one another.
\end{question}

A Bernoulli trial is a coin flip where the probability of heads is $p$ and the probability of tails is $1-p$.  If $p$ is allowed to change from one independent Bernoulli trial to the next, then we have a sequence of Poisson trials.  

\begin{question}
Why do the indicator random variables represent Poisson trials and not Bernoulli trials?
\end{question}

\noindent Since the random indicator variables above represent $n$ Poisson trials, we can apply the Chernoff bound from Lemma 1.2\footnote{This is Theorem 4.1 in Motwani and Raghavan} of the de Berg probability handout.  We restate the Lemma here for completeness.

\begin{lemma}[Chernoff Bound]
Let $X_{1}, X_{2}, \ldots, X_{n}$ be independent Poisson trials such that, for $1 \leq i \leq n$, $Pr[X_{1}=1] = p_{i}$, where $0 < p_{i} < 1$.  Then, for $X = \sum_{i=1}^{n} X_{i}$, $\mu = E[X] = \sum_{x=1}^{n} p_{i}$ and any $\delta > 0$,
\[
Pr[X > (1+\delta)\mu] \leq \left( \frac{e^{\delta}}{(1+\delta)^{(1+\delta)}} \right)^{\mu}.
\]
\end{lemma}

\noindent If $1+\delta$ is a sufficiently large constant then the Chernoff bound says that the probability that $X$ deviates largely from its expectation decreases exponentially in the expectation. 

\begin{problem}
Choose $\delta=e^{2}-1$ and use the fact that $H_{k} + H_{n-k+1} \geq H_{n} \geq \ln n$ to show that 
\[
Pr[depth(x_{k}) \leq 32 \ln n] \geq 1-(1/n^{6}).
\]
Hint:  use the Chernoff bound from above to show that $Pr[depth(x_{k}) > 32 \ln n] \leq 1/n^{6}$.
\end{problem}

We have now shown that every $x_{i} \in S$ has depth $O(\log n)$ with probability at least $1-(1/n^{6})$.  However, we want this to be true for every node in $S$.  For this, we use a probabilistic tool called the {\em union bound}.  The union bound says that, given a bunch of events, the probability of at least one of them happening is at most the sum of the probabilities of any of them happening.  In other words, if $Y_{1}, Y_{2}, \ldots, Y_{n}$ are events, then
\[
Pr[Y_{1} \cup Y_{2} \cup \cdots \cup Y_{n}] \leq Pr[Y_{1}] + Pr[Y_{2}] + \cdots + Pr[Y_{n}].
\]
We often use the union bound to bound bad events, so if each $Y_{i}$ is a bad event that occurs with some small probability, then the probability that any of bad events occurs is still pretty small.  So, if we take $Y_{i}$ to be the event that $depth(x_{i}) > 32 \ln n$ then the probability that at least one of the nodes has depth larger than $32 \ln n$ is at most $n \cdot (1/ n^{6})= 1/n^{5}$.  Thus, the probability that the whole tree has depth at most $32 \ln n$ is $1-1/n^{5}$.  That's pretty awesome.


\section{Theory vs. Practice}

Often there is a disconnect between theory and practice.  We will very much try to remedy this gap by discussing the feasibility of certain algorithm and data structure implementations.  Our first foray into this chasm is some empirical work to both confirm our analysis from above and experimentally validate the claim that a Treap is a competitive alternative to the standard, deterministic balanced binary search trees.

\begin{problem}
You will implement a treap data structure in Java and compare it with the the {\tt TreeSet} class in Java which, as we already mentioned, is a red-black tree implementation of a set.  Read the description of the Java {\tt TreeSet} at \url{http://download.oracle.com/javase/6/docs/api/}.  Notice that its interface includes the method {\tt add} for insertion, the method {\tt ceiling} for successor, the method {\tt floor} for predecessor, the method {\tt remove} for delete, and the method {\tt contains} which is essentially search since this set class assumes (rightfully so) that two elements with the same key are actually the same.  You'll notice that the {\tt TreeMap} class (on which the {\tt TreeSet} class is based) has a {\tt get} method which retrieves the {\em value} for a {\em key}.  We will base our treap interface on the interface for {\tt TreeSet}.  You should perform the following:  

\begin{itemize}
	\item Implement a Treap data structure, parameterized by type {\tt E},  in a file called {\tt Treap.java}.  The class should have at least three constructors, a default constructor which constructs a new treap, sorted according to the natural ordering of its elements, one which accepts a {\tt Comparator}, and one which accepts a Java {\tt Collection} and constructs a new treap containing the elements in the specified collection, sorted according to the natural ordering of the elements.  You'll see these are essentially the same constructors as the first three listed on the Java doc page.
	\item Provide methods for {\tt add}, {\tt ceiling}, {\tt remove}, {\tt floor}, and {\tt contains}.
	\item Perform the following experiment:  Generate 50000 random integers and insert them it into both the treap and the {\tt TreeSet}.  Record the time it took to perform the insertion (over groups of, say, 1000) for both data structures.  Making sure to not delete the integers from the sets, repeat this process until both data structures contain the same random integers.  Now create a graph where size is on the horizontal axis and time is on the vertical axis and the performance of the two data structures is compared.  How did the treap perform?  Does the running time scale logarithmically with the size?
	\item Perform one more experiment of your own choosing.  Provide a clear, complete explanation of your results.  Please include graphs and figures where appropriate.
	\item Implement your $split(T,k)$ operation from Problem~\ref{prob:split} and provide some empirical evidence that it runs in time proportional to the height of the tree.
	\item Please turn in your code using the {\tt turnin} command on the unix machines.
\end{itemize}
\end{problem}

\end{document}
